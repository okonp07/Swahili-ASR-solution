{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Swahili ASR — Offline Baseline (Colab)\n",
        "\n",
        "**Goal:** Build a fast, privacy-preserving Kiswahili ASR baseline that runs offline on a single NVIDIA T4 (≤16 GB), produces `submission.csv`, and logs Real‑Time Factor (RTFx).\n",
        "\n",
        "> ⚠️ **Use only Zindi-provided data** for training/evaluation. Pretrained *open* models are okay unless the challenge page states otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1) Install dependencies (open-source only)\n",
        "!pip -q install 'faster-whisper>=1.0.0' ctranslate2 datasets soundfile jiwer torch torchaudio pynvml --upgrade\n",
        "!pip -q install transformers accelerate evaluate sentencepiece pyctcdecode\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2) Config\n",
        "import os, re, time, json, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "SEED = 42\n",
        "MODEL_NAME = 'small'          # tiny|base|small\n",
        "COMPUTE_TYPE = 'int8_float16' # good balance of speed/memory on T4\n",
        "LANGUAGE = 'sw'\n",
        "BEAM_SIZE = 5; PATIENCE = 0.2; TEMPERATURE = 0.0\n",
        "VAD_FILTER = True; MIN_SIL_MS = 200; CONDITION_ON_PREV = False\n",
        "TEST_WAV_GLOB = 'data/test/*.wav'  # put Zindi test wavs here\n",
        "OUT_CSV = 'artifacts/submission.csv'\n",
        "\n",
        "os.makedirs('artifacts', exist_ok=True)\n",
        "\n",
        "def normalize_text(t: str) -> str:\n",
        "  t = t.lower()\n",
        "  t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
        "  t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "  return t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Data layout help\n",
        "Place files like so:\n",
        "```\n",
        "data/\n",
        " ├── test/               # Zindi test WAV files\n",
        " ├── train/              # Zindi train WAV files\n",
        " └── splits/             # your own manifests (filename,text) for val\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3) Load model (faster-whisper / CTranslate2)\n",
        "model = WhisperModel(MODEL_NAME, device='cuda', compute_type=COMPUTE_TYPE)\n",
        "print('Loaded model:', MODEL_NAME, 'compute_type:', COMPUTE_TYPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4) Inference on test set -> submission.csv\n",
        "wav_files = sorted(glob.glob(TEST_WAV_GLOB))\n",
        "rows, rtfs = [], []\n",
        "for p in wav_files:\n",
        "  audio, sr = sf.read(p)\n",
        "  clip_len = len(audio) / float(sr)\n",
        "  t0 = time.time()\n",
        "  segments, info = model.transcribe(\n",
        "      audio,\n",
        "      language=LANGUAGE, beam_size=BEAM_SIZE, patience=PATIENCE,\n",
        "      vad_filter=VAD_FILTER, vad_parameters=dict(min_silence_duration_ms=MIN_SIL_MS),\n",
        "      condition_on_previous_text=CONDITION_ON_PREV, temperature=TEMPERATURE,\n",
        "  )\n",
        "  hypo = ' '.join(s.text for s in segments).strip()\n",
        "  elapsed = time.time() - t0\n",
        "  rtf = clip_len / max(elapsed, 1e-6)\n",
        "  rtfs.append(rtf)\n",
        "  rows.append({'filename': os.path.basename(p), 'text': normalize_text(hypo)})\n",
        "  print(os.path.basename(p), f'len={clip_len:.2f}s time={elapsed:.2f}s RTFx={rtf:.2f}')\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(rows).to_csv(OUT_CSV, index=False)\n",
        "print('Saved submission ->', OUT_CSV)\n",
        "print('Mean RTFx:', np.mean(rtfs))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5) (Optional) WER on a local validation split\n",
        "import jiwer\n",
        "VAL_MANIFEST = 'data/splits/val_manifest.csv'  # filename,text\n",
        "if os.path.exists(VAL_MANIFEST):\n",
        "  df = pd.read_csv(VAL_MANIFEST)\n",
        "  preds, refs = [], []\n",
        "  for _, row in df.iterrows():\n",
        "    wav_p = os.path.join('data','train', row['filename'])\n",
        "    if not os.path.exists(wav_p):\n",
        "      continue\n",
        "    audio, sr = sf.read(wav_p)\n",
        "    segments, _ = model.transcribe(\n",
        "        audio, language=LANGUAGE, beam_size=BEAM_SIZE, patience=PATIENCE,\n",
        "        vad_filter=VAD_FILTER, vad_parameters=dict(min_silence_duration_ms=MIN_SIL_MS),\n",
        "        condition_on_previous_text=CONDITION_ON_PREV, temperature=TEMPERATURE,\n",
        "    )\n",
        "    hypo = normalize_text(' '.join(s.text for s in segments).strip())\n",
        "    ref  = normalize_text(str(row['text']))\n",
        "    preds.append(hypo); refs.append(ref)\n",
        "  print('Validation WER:', jiwer.wer(refs, preds))\n",
        "else:\n",
        "  print('Skip: no validation manifest at', VAL_MANIFEST)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6) Peak GPU memory (approx) — ensure < 16 GB\n",
        "import subprocess, re\n",
        "try:\n",
        "  out = subprocess.check_output(['bash','-lc','nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits']).decode()\n",
        "  vals = [int(x) for x in re.split(r'[\\r\\n]+', out) if x.strip()]\n",
        "  print('Approx GPU memory used (MiB):', max(vals) if vals else 'n/a')\n",
        "except Exception as e:\n",
        "  print('nvidia-smi not available:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Fine-tune CTC (Wav2Vec2 XLS‑R)\n",
        "Use `scripts/train_wav2vec2_ctc.py` with your own `data/splits/train_manifest.csv` and `val_manifest.csv`. Keep batches small to fit a single T4; prefer FP16.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}